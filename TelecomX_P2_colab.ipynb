{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49fa000c",
   "metadata": {},
   "source": [
    "# Telecom X — Parte 2 · Predicción de Churn (Colab-only)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/flacoca1970/Desafio_2_P2/blob/main/notebooks/TelecomX_P2_colab.ipynb)\n",
    "\n",
    "Este **cuaderno único** ejecuta todo el flujo de **preprocesamiento, análisis, modelado, evaluación, calibración y selección de umbral de negocio** para predecir la cancelación (churn).\n",
    "**Requisito**: usar el **CSV tratado** de la Parte 1 (`df_limpo.csv`).\n",
    "\n",
    "> Tip: sube tu `df_limpo.csv` a `/content` o indica la ruta exacta abajo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670dc3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dependencias (solo en Colab) ===\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip -q install imbalanced-learn >/dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03777a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports y configuración ===\n",
    "import os, warnings, io, json\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, precision_recall_curve, roc_curve,\n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Opcional: oversampling (solo en train)\n",
    "USE_BALANCING = False      # True para activar RandomOverSampler\n",
    "BALANCING_METHOD = 'ros'   # 'ros' (RandomOverSampler) o 'smote'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parámetros de entrada/salida ===\n",
    "# Ruta del CSV tratado de la Parte 1\n",
    "CSV_PATH = '/content/df_limpo.csv'     # cambia si lo tienes en otra ruta\n",
    "# Parámetros de negocio para el umbral\n",
    "VALUE_RETAIN = 100.0\n",
    "COST_CONTACT = 5.0\n",
    "\n",
    "# Donde guardar artefactos\n",
    "os.makedirs('/content/reports', exist_ok=True)\n",
    "os.makedirs('/content/data/interim', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f80789",
   "metadata": {},
   "source": [
    "## 1) Carga del dataset tratado (Parte 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dcff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el CSV tratado (df_limpo)\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f'No se encontró el CSV tratado en: {CSV_PATH}. Sube df_limpo.csv o ajusta CSV_PATH.')\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print('Shape df:', df.shape)\n",
    "display(df.head(3))\n",
    "# Copia de seguridad en la estructura del proyecto (opcional)\n",
    "df.to_csv('/content/data/interim/df_limpo.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093ab99",
   "metadata": {},
   "source": [
    "## 2) Preparación de datos (validación de target, split, pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar target y limpiar filas no válidas para modelado\n",
    "assert 'Churn' in df.columns, \"El CSV tratado debe incluir la columna 'Churn'.\"\n",
    "\n",
    "y_raw = df['Churn'].astype('string').str.strip().str.lower()\n",
    "valid_mask = y_raw.isin(['yes','no'])\n",
    "print(f'Filas totales: {len(y_raw)} | válidas (yes/no): {valid_mask.sum()} | inválidas/NaN: {(~valid_mask).sum()}')\n",
    "\n",
    "dfm = df.loc[valid_mask].copy()\n",
    "dfm['Churn_bin'] = dfm['Churn'].astype('string').str.strip().str.lower().map({'yes':1,'no':0})\n",
    "\n",
    "# Definir X y y\n",
    "X = dfm.drop(columns=['Churn','Churn_bin','customerID'], errors='ignore')\n",
    "y = dfm['Churn_bin'].to_numpy()\n",
    "\n",
    "# Columnas numéricas y categóricas\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print('Num cols:', len(num_cols), '| Cat cols:', len(cat_cols))\n",
    "\n",
    "# Pipelines\n",
    "num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                     ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "pre = ColumnTransformer([('num', num_pipe, num_cols),\n",
    "                         ('cat', cat_pipe, cat_cols)])\n",
    "\n",
    "# Split estratificado (antes de cualquier balanceo)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)\n",
    "\n",
    "# Oversampling opcional (solo en train)\n",
    "if USE_BALANCING:\n",
    "    from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "    sampler = RandomOverSampler(random_state=RANDOM_STATE) if BALANCING_METHOD=='ros' else SMOTE(random_state=RANDOM_STATE)\n",
    "    Xt = pre.fit_transform(X_train)\n",
    "    X_train_bal, y_train_bal = sampler.fit_resample(Xt, y_train)\n",
    "    PRE_FITTED = pre.fit(X_train, y_train)\n",
    "    X_train_array, y_train_array = X_train_bal, y_train_bal\n",
    "else:\n",
    "    PRE_FITTED, X_train_array, y_train_array = None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29cbfe",
   "metadata": {},
   "source": [
    "## 3) Análisis de correlación / selección de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920eb2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Punto-biserial aprox (Pearson con target binario) para numéricas\n",
    "def safe_corr(a, b):\n",
    "    try:\n",
    "        return float(pd.Series(a).corr(pd.Series(b)))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "corrs = []\n",
    "for c in num_cols:\n",
    "    corrs.append((c, safe_corr(dfm[c], dfm['Churn_bin'])))\n",
    "corrs = sorted(corrs, key=lambda x: (abs(0 if x[1] is None else x[1])), reverse=True)\n",
    "print('Top 10 correlaciones numéricas con churn:')\n",
    "for c,v in corrs[:10]:\n",
    "    print(f\"{c:35s} {v:.4f}\" if v==v else f\"{c:35s} NaN\")\n",
    "\n",
    "# 3.2 Información Mutua (mixta: num + cat)\n",
    "X_for_mi = pre.fit_transform(X)\n",
    "mi = mutual_info_classif(X_for_mi, y, discrete_features=False, random_state=RANDOM_STATE)\n",
    "\n",
    "def get_feature_names(pre, num_cols, cat_cols):\n",
    "    out = []\n",
    "    if 'num' in pre.named_transformers_ and pre.named_transformers_['num'] is not None:\n",
    "        out.extend(num_cols)\n",
    "    if 'cat' in pre.named_transformers_ and pre.named_transformers_['cat'] is not None:\n",
    "        onehot = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "        out.extend(onehot.get_feature_names_out(cat_cols).tolist())\n",
    "    return out\n",
    "feat_names = get_feature_names(pre, num_cols, cat_cols)\n",
    "mi_pairs = list(zip(feat_names, mi))\n",
    "mi_pairs = sorted(mi_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Plot MI (top 20)\n",
    "top = mi_pairs[:20]\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh([n for n,_ in reversed(top)], [v for _,v in reversed(top)])\n",
    "plt.xlabel('Mutual Information'); plt.title('Top 20 features por MI'); plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cbd99",
   "metadata": {},
   "source": [
    "## 4) Modelos y GridSearch (métrica principal: PR-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65dc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "models = {}\n",
    "models['dummy'] = (DummyClassifier(strategy='stratified', random_state=RANDOM_STATE), {'model__strategy':['stratified']})\n",
    "models['logreg'] = (LogisticRegression(max_iter=2000, class_weight='balanced'), {'model__C':[0.1,1,5,10]})\n",
    "models['rf'] = (RandomForestClassifier(n_estimators=400, n_jobs=-1, random_state=RANDOM_STATE, class_weight='balanced'),\n",
    "                {'model__max_depth':[None,8,14], 'model__min_samples_leaf':[1,3,6]})\n",
    "models['gb'] = (GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "                {'model__n_estimators':[200,400], 'model__learning_rate':[0.05,0.1], 'model__max_depth':[2,3]})\n",
    "models['hgb'] = (HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "                 {'model__max_depth':[None,10,14], 'model__learning_rate':[0.05,0.1]})\n",
    "results = {}\n",
    "best_est, best_name, best_score = None, None, -np.inf\n",
    "for name, (est, grid) in models.items():\n",
    "    pipe = Pipeline([('pre', pre), ('model', est)])\n",
    "    gs = GridSearchCV(pipe, param_grid=grid, scoring='average_precision', cv=cv, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    results[name] = gs.best_score_\n",
    "    if gs.best_score_ > best_score:\n",
    "        best_est, best_name, best_score = gs.best_estimator_, name, gs.best_score_\n",
    "print('PR-AUC (CV) por modelo:')\n",
    "for k,v in results.items():\n",
    "    print(f\"{k:8s} -> {v:.4f}\")\n",
    "print(f\"Mejor: {best_name} (PR-AUC CV={best_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737345d7",
   "metadata": {},
   "source": [
    "## 5) Calibración y evaluación en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb542cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibración (isotónica vs platt)\n",
    "pre_fitted = best_est.named_steps['pre'].fit(X_train, y_train)\n",
    "Xtr = pre_fitted.transform(X_train)\n",
    "Xte = pre_fitted.transform(X_test)\n",
    "base = best_est.named_steps['model']\n",
    "cal_used = 'isotonic'\n",
    "try:\n",
    "    cal_iso = CalibratedClassifierCV(base, method='isotonic', cv=3).fit(Xtr, y_train)\n",
    "    y_score = cal_iso.predict_proba(Xte)[:,1]\n",
    "    cal_used = 'isotonic'\n",
    "except Exception:\n",
    "    try:\n",
    "        cal_platt = CalibratedClassifierCV(base, method='sigmoid', cv=3).fit(Xtr, y_train)\n",
    "        y_score = cal_platt.predict_proba(Xte)[:,1]\n",
    "        cal_used = 'platt'\n",
    "    except Exception:\n",
    "        if hasattr(base, 'predict_proba'):\n",
    "            y_score = base.fit(Xtr, y_train).predict_proba(Xte)[:,1]\n",
    "            cal_used = 'none_proba'\n",
    "        else:\n",
    "            y_score = base.fit(Xtr, y_train).decision_function(Xte)\n",
    "            y_min, y_max = y_score.min(), y_score.max()\n",
    "            y_score = (y_score - y_min) / (y_max - y_min + 1e-9)\n",
    "            cal_used = 'none_decision_scaled'\n",
    "roc = roc_auc_score(y_test, y_score)\n",
    "pr  = average_precision_score(y_test, y_score)\n",
    "print(f'ROC-AUC (test): {roc:.4f} | PR-AUC (test): {pr:.4f} | Calibración: {cal_used}')\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_score)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "plt.figure(figsize=(6,4)); plt.plot(rec, prec); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall'); plt.show()\n",
    "plt.figure(figsize=(6,4)); plt.plot(fpr, tpr); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39026de",
   "metadata": {},
   "source": [
    "## 6) Umbral de negocio y matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_profit(y_true, y_score, thr, value=VALUE_RETAIN, cost=COST_CONTACT):\n",
    "    y_pred = (y_score >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tp*value - (tp+fp)*cost\n",
    "thr_grid = np.linspace(0.01, 0.99, 99)\n",
    "profits = [business_profit(y_test, y_score, t) for t in thr_grid]\n",
    "best_thr = float(thr_grid[int(np.argmax(profits))])\n",
    "best_profit = float(np.max(profits))\n",
    "print('Umbral óptimo:', round(best_thr,3), '| Ganancia estimada:', round(best_profit,2))\n",
    "plt.figure(figsize=(6,4)); plt.plot(thr_grid, profits); plt.axvline(best_thr, linestyle='--'); plt.xlabel('Threshold'); plt.ylabel('Profit'); plt.title('Curva de Ganancia'); plt.show()\n",
    "y_pred = (y_score >= best_thr).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de confusión (umbral negocio):\\n', cm)\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52fb912",
   "metadata": {},
   "source": [
    "## 7) Interpretabilidad (coeficientes / importancias / permutación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48301493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(pre, num_cols, cat_cols):\n",
    "    out = []\n",
    "    if 'num' in pre.named_transformers_ and pre.named_transformers_['num'] is not None:\n",
    "        out.extend(num_cols)\n",
    "    if 'cat' in pre.named_transformers_ and pre.named_transformers_['cat'] is not None:\n",
    "        onehot = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "        out.extend(onehot.get_feature_names_out(cat_cols).tolist())\n",
    "    return out\n",
    "feat_names = get_feature_names(pre_fitted, num_cols, cat_cols)\n",
    "# 7.1 Coeficientes (si el modelo es lineal)\n",
    "try:\n",
    "    fitted = best_est.set_params().fit(X_train, y_train)\n",
    "    clf = fitted.named_steps['model']\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        coefs = clf.coef_.ravel() if getattr(clf, 'coef_', None) is not None else None\n",
    "        if coefs is not None:\n",
    "            names = get_feature_names(fitted.named_steps['pre'], num_cols, cat_cols)\n",
    "            pairs = sorted(zip(names, coefs), key=lambda x: abs(x[1]), reverse=True)[:20]\n",
    "            nn, vv = zip(*pairs)\n",
    "            plt.figure(figsize=(8,5)); plt.barh(list(reversed(nn)), list(reversed(vv))); plt.title('Top 20 coeficientes (abs)'); plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print('Coeficientes no disponibles:', e)\n",
    "# 7.2 Feature importances (árboles)\n",
    "try:\n",
    "    if hasattr(base, 'feature_importances_'):\n",
    "        imps = base.fit(pre_fitted.transform(X_train), y_train).feature_importances_\n",
    "        pairs = sorted(zip(feat_names, imps), key=lambda x: x[1], reverse=True)[:20]\n",
    "        nn, vv = zip(*pairs)\n",
    "        plt.figure(figsize=(8,5)); plt.barh(list(reversed(nn)), list(reversed(vv))); plt.title('Top 20 importancias (modelo)'); plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print('Importancias no disponibles:', e)\n",
    "# 7.3 Permutation importance\n",
    "try:\n",
    "    perm = permutation_importance(best_est.set_params().fit(X_train, y_train), X_test, y_test, scoring='average_precision', n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    nn, vv = zip(*sorted(zip(X.columns, perm.importances_mean), key=lambda x: x[1], reverse=True)[:20])\n",
    "    plt.figure(figsize=(8,5)); plt.barh(list(reversed(nn)), list(reversed(vv))); plt.title('Top 20 importancias por permutación (PR-AUC)'); plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print('Permutation importance no disponible:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efc3f6",
   "metadata": {},
   "source": [
    "## 8) Artefactos: metrics.json, informe P2 y export Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar métricas clave\n",
    "metrics = {\n",
    "    'best_model': str(best_name),\n",
    "    'cv_best_pr_auc': float(best_score),\n",
    "    'test_roc_auc': float(roc_auc_score(y_test, y_score)),\n",
    "    'test_pr_auc': float(average_precision_score(y_test, y_score)),\n",
    "    'business_best_threshold': float(best_thr),\n",
    "    'business_best_profit': float(best_profit),\n",
    "    'value_retain': float(VALUE_RETAIN),\n",
    "    'cost_contact': float(COST_CONTACT),\n",
    "    'calibration': cal_used\n",
    "}\n",
    "with open('/content/reports/metrics_p2.json','w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print('Guardado /content/reports/metrics_p2.json')\n",
    "\n",
    "# Informe Markdown P2\n",
    "lines = []\n",
    "lines.append('# Informe de Resultados — Telecom X · Parte 2')\n",
    "lines.append('')\n",
    "lines.append('## 1. Datos y preparación')\n",
    "lines.append(f'- Filas: **{df.shape[0]}** | Columnas: **{df.shape[1]}**')\n",
    "lines.append(f'- Columnas numéricas: **{len(num_cols)}** | categóricas: **{len(cat_cols)}**')\n",
    "lines.append('- Split: **train/test 80/20** estratificado; oversampling: **%s**' % ('Sí' if USE_BALANCING else 'No'))\n",
    "lines.append('')\n",
    "lines.append('## 2. Selección de variables (resumen)')\n",
    "lines.append('- Top 10 numéricas por correlación (abs):')\n",
    "for c,v in (corrs[:10] if isinstance(corrs, list) else []):\n",
    "    lines.append(f'  - {c}: {v:.4f}' if v==v else f'  - {c}: NaN')\n",
    "lines.append('- Top 20 features por Información Mutua (ver gráfico en el notebook).')\n",
    "lines.append('')\n",
    "lines.append('## 3. Modelado y evaluación')\n",
    "lines.append(f'- **Mejor modelo (CV por PR-AUC)**: **{best_name}** con **{best_score:.4f}**')\n",
    "lines.append(f'- En test: **ROC-AUC**={roc:.4f}, **PR-AUC**={pr:.4f}, **calibración**={cal_used}')\n",
    "lines.append('')\n",
    "lines.append('## 4. Umbral y negocio')\n",
    "lines.append(f'- Threshold óptimo: **{best_thr:.3f}** → Ganancia estimada: **{best_profit:.2f}** (VALUE_RETAIN={VALUE_RETAIN}, COST_CONTACT={COST_CONTACT})')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, (y_score>=best_thr).astype(int)).ravel()\n",
    "lines.append('')\n",
    "lines.append('**Matriz de confusión (umbral negocio):**')\n",
    "lines.append('')\n",
    "lines.append('|       | Pred No | Pred Yes |')\n",
    "lines.append('|-------|---------|----------|')\n",
    "lines.append(f'| Real No  | {tn} | {fp} |')\n",
    "lines.append(f'| Real Yes | {fn} | {tp} |')\n",
    "lines.append('')\n",
    "lines.append('## 5. Conclusiones')\n",
    "lines.append('- Factores con mayor influencia alineados con la Parte 1 (contrato mes a mes, e-check, fibra).')\n",
    "lines.append('- El umbral maximiza **ganancia**, priorizando recall si `VALUE_RETAIN` es alto respecto a `COST_CONTACT`.')\n",
    "lines.append('- Recomendación: campañas por segmento con anualización, migración a auto-pay y soporte fibra proactivo.')\n",
    "with open('/content/reports/README_REPORT_P2.md','w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(lines))\n",
    "print('Guardado /content/reports/README_REPORT_P2.md')\n",
    "\n",
    "# Export Top N para CRM\n",
    "N = 500\n",
    "salida = dfm.loc[X_test.index].copy()\n",
    "salida['churn_score'] = y_score\n",
    "salida['flag_churn_risk'] = (y_score >= best_thr).astype(int)\n",
    "topN = salida.sort_values('churn_score', ascending=False).head(N)\n",
    "top_path = '/content/reports/clientes_en_riesgo_topN_p2.csv'\n",
    "topN.to_csv(top_path, index=False)\n",
    "print('Exportado:', top_path)\n",
    "try:\n",
    "    display(topN.head(10))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
